{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbafd32",
   "metadata": {},
   "source": [
    "# Real Data Time Series Forecasting: Model 1 (Bayesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757904f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils_model1\n",
    "\n",
    "importlib.reload(utils_model1)\n",
    "from utils_model1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf10409",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda_check(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13295c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('DeDe_48stations/DeDe_48sites_metadata.csv')\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37879843",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('DeDe_48stations_allfeatures/GHI_CI_NCEP_Iclr_15min_DeDe2023_newiclr.csv', index_col='Datetime', parse_dates=True)\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop(columns=['CI_CM', 'CI_RGB', 'Wind direction', 'Snowfall', 'Snow depth'])\n",
    "df = df.rename(columns={\n",
    "    'Relative Humidity': 'RelativeHumidity', \n",
    "    'Wind speed': 'WindSpeed',\n",
    "    'Short-wave irradiation': 'Inwp',\n",
    "    })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a653083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site1 = df[df['Site_id'] == 1].drop(columns = 'Site_id')\n",
    "\n",
    "#----- Visualizing Missing Values in DataFrame -----\n",
    "# Visualize missing data matrix\n",
    "msno.matrix(df_site1)\n",
    "plt.title(\"Missing Data Matrix\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "''' \n",
    "The rightmost line is a sparkline that summarizes the completeness of data in each rows. \n",
    "- number '0' means 'there are 0 column that have no missing values in all rows'\n",
    "- number '10' means 'there are at most 10 columns that have no missing values in the same row'\n",
    "'''\n",
    "\n",
    "# Visualize missing data bar chart\n",
    "msno.bar(df_site1)\n",
    "plt.title(\"Missing Data Bar Chart\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15e04b",
   "metadata": {},
   "source": [
    "There are 2 types of missing data (gap):\n",
    "1. Small gaps: less than 12 hours, we will use 'Linear Interpolation' to fill in the missing values in each column.\n",
    "2. Large gaps: more than or equal 12 hours, we will leave them as NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205cca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full date range\n",
    "starting_date = min(df_site1.index)\n",
    "ending_date = max(df_site1.index)\n",
    "resolution = df_site1.index[1] - df_site1.index[0]\n",
    "\n",
    "full_date_range = pd.date_range(start=starting_date, end=ending_date, freq=resolution)\n",
    "\n",
    "# Make index of df_site1 have consistent resolution (where the missing data are filled with NaN to be imputed later)\n",
    "df_site1 = df_site1.reindex(full_date_range)\n",
    "\n",
    "# Add Hour index into  the dataframe\n",
    "df_site1['Hour'] = np.sin(df_site1.index.hour / 24  * np.pi)\n",
    "\n",
    "df_site1 = df_site1.drop(columns=['CI_CM_interpolated', 'CI_RGB_interpolated', 'rawI'])\n",
    "\n",
    "# Drop nan rows\n",
    "df_site1 = df_site1['2023-01-01 07:15:00':]\n",
    "\n",
    "# Physically, irradiance should not exceed $1200 \\text{ W/m}^2$ \n",
    "# since it is the solar irradiance that is measured from atmosphere. \n",
    "# We can see that there are some values that exceed this limit.\n",
    "# We will set (or so called 'clip') these values to $1200 \\text{ W/m}^2$.\n",
    "\n",
    "df_site1.loc[:, 'I'] = df_site1['I'].clip(upper=1200)\n",
    "df_site1.loc[:, 'Inwp'] = df_site1['Inwp'].clip(upper=1200)\n",
    "df_site1.loc[:, 'Iclr'] = df_site1['Iclr'].clip(upper=1200)\n",
    "\n",
    "# df_site1['CI_RGB_interpolated'] = df_site1['CI_RGB_interpolated']  / 255\n",
    "# in the column 'Temperature', the outlier occurs in 2023-01-30 between 04:00 and 10:00.\n",
    "\n",
    "df_site1.loc['2023-01-30 04:00': '2023-01-30 10:00', 'Temperature'] = np.nan\n",
    "\n",
    "#  Create a NaN mask\n",
    "cols = ['I', 'Temperature', 'RelativeHumidity', 'Pressure', 'WindSpeed', 'Rainfall', 'Inwp', 'Iclr']\n",
    "\n",
    "for col in cols:\n",
    "    nan_mask = df_site1[col].isna()\n",
    "    \n",
    "    # Identify consecutive NaN groups\n",
    "    group_id = (nan_mask != nan_mask.shift()).cumsum(axis=0)  # Unique ID for each block per column\n",
    "\n",
    "    # Prepare empty DataFrame to store group sizes per column\n",
    "    group_sizes = nan_mask.groupby(group_id).transform('sum')\n",
    "\n",
    "    # Build interpolation mask: interpolate only gaps \n",
    "    interpolation_mask = nan_mask & (group_sizes < 48)  # 48 time steps = 48 * 15 minutes = 12 hours\n",
    "\n",
    "    # Temporarily mask large gaps\n",
    "    temp_df = df_site1[col].copy()\n",
    "\n",
    "    # Perform linear time-based interpolation for all columns\n",
    "    interpolated_df = temp_df.interpolate(method='time', axis=0).where(interpolation_mask)\n",
    "\n",
    "    # Assign interpolated results back to the original dataframe\n",
    "    df_site1[col] = df_site1[col].fillna(interpolated_df)\n",
    "\n",
    "rows_with_nans = df_site1.isna().any(axis=1)\n",
    "df_site1.loc[rows_with_nans] = np.nan\n",
    "\n",
    "df_site1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666861a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Visualizing Missing Values in DataFrame -----\n",
    "\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize missing data matrix\n",
    "msno.matrix(df_site1)\n",
    "plt.title(\"Missing Data Matrix\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "''' \n",
    "The rightmost line is a sparkline that summarizes the completeness of data in each rows. \n",
    "- number '0' means 'there are 0 column that have no missing values in all rows'\n",
    "- number '10' means 'there are at most 10 columns that have no missing values in the same row'\n",
    "'''\n",
    "\n",
    "# Visualize missing data bar chart\n",
    "msno.bar(df_site1)\n",
    "plt.title(\"Missing Data Bar Chart\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset (at site 1)\n",
    "\n",
    "columns = ['I', 'Inwp', 'Iclr']\n",
    "columns_name_text = ['I', 'I_nwp', 'I_clr']\n",
    "\n",
    "# Visualize the result\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the 'Actual' trace\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    fig.add_trace(go.Scatter(x=df_site1.index, y=df_site1[columns[i]],\n",
    "                         mode='lines',\n",
    "                         name=columns_name_text[i],\n",
    "                         line=dict(width=1)))\n",
    "\n",
    "# Update layout for title, axis labels, and grid\n",
    "fig.update_layout(\n",
    "    title='Time Series Plot',\n",
    "    xaxis_title='DateTime',\n",
    "    yaxis_title='Irradiance',\n",
    "    hovermode='x unified', # This is useful for time series to see values across traces at a given x\n",
    "    template='plotly_white' # A clean white background template\n",
    ")\n",
    "\n",
    "# Add gridlines\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69815926",
   "metadata": {},
   "source": [
    "### EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for all columns\n",
    "correlation_matrix = df_site1.corr(method='pearson')\n",
    "\n",
    "# Get the columns to plot against 'I', excluding 'I' itself\n",
    "columns_to_plot = df_site1.drop(columns=['I']).columns\n",
    "\n",
    "# Create subplots dynamically\n",
    "# The number of subplots is equal to the number of columns to plot against 'I'\n",
    "fig, ax = plt.subplots(1, len(columns_to_plot), figsize=(5 * len(columns_to_plot), 5))\n",
    "\n",
    "# If there's only one column to plot, ax will not be an array, so handle that case\n",
    "if len(columns_to_plot) == 1:\n",
    "    ax = [ax] # Make it a list so the loop works consistently\n",
    "\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    # Calculate the Pearson correlation coefficient between 'I' and the current column\n",
    "    # We use .loc to ensure we get the specific correlation value\n",
    "    correlation = correlation_matrix.loc['I', col]\n",
    "\n",
    "    # Create the scatter plot\n",
    "    ax[i].scatter(df_site1['I'], df_site1[col], c='black', alpha=0.2)\n",
    "    \n",
    "    # Set labels\n",
    "    ax[i].set_xlabel('$I$')\n",
    "    ax[i].set_ylabel(f'{col}')\n",
    "    \n",
    "    # Set title including the correlation value\n",
    "    # Using f-string for easy formatting, rounding correlation to 2 decimal places\n",
    "    ax[i].set_title(f'$I$ vs. {col}\\n(Corr: {correlation:.2f})')\n",
    "\n",
    "plt.tight_layout() # Adjusts subplot parameters for a tight layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabce06",
   "metadata": {},
   "source": [
    "### Make an input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c64b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Input Sequences\n",
    "lookback = pd.Timedelta('1d')\n",
    "lookforward = pd.Timedelta('4d')\n",
    "\n",
    "lookback_timestep = lookback // (df_site1.index[1] - df_site1.index[0])\n",
    "lookforward_timestep = lookforward // (df_site1.index[1] - df_site1.index[0])\n",
    "num_feature = df_site1.shape[1]\n",
    "\n",
    "lag_regressor_cols = ['I']\n",
    "target_cols=['I']\n",
    "\n",
    "x_numpy, y_numpy, x_labels, y_labels, datetime = df_transform(df = df_site1, lag_regressor_cols=lag_regressor_cols, target_cols=target_cols, lookback_timedelta=lookback, lookforward_timedelta = lookforward)\n",
    "\n",
    "x_scaled_numpy, y_scaled_numpy, x_scalers, y_scaler  = scale_transform(x_numpy, y_numpy)\n",
    "\n",
    "x_scaled = torch.tensor(x_scaled_numpy, dtype=torch.float32).to(device)\n",
    "y_scaled = torch.tensor(y_scaled_numpy, dtype=torch.float32).to(device)\n",
    "\n",
    "# Train-Val-Test splitting\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "train_samples_count = int(x_scaled.shape[0] * train_ratio)\n",
    "val_samples_count = int(x_scaled.shape[0] * val_ratio)\n",
    "\n",
    "x_train_scaled, y_train_scaled, \\\n",
    "x_val_scaled, y_val_scaled, \\\n",
    "x_test_scaled, y_test_scaled, \\\n",
    "datetime_train, datetime_val, datetime_test = split_time_series_data(x = x_scaled, y = y_scaled, train_ratio = train_ratio, val_ratio = val_ratio, datetime=datetime)\n",
    "\n",
    "\n",
    "# Make a dataloader for train dataset \n",
    "train_dataset = TensorDataset(x_train_scaled, y_train_scaled)\n",
    "val_dataset = TensorDataset(x_val_scaled, y_val_scaled)\n",
    "test_dataset = TensorDataset(x_test_scaled, y_test_scaled)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle = False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 32, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "# The issue is from shuffling mechanics of torch making data in train_dataloader locates in cpu instead of gpu which we don't want to, so we apply generator manually.\n",
    "# Create a generator for reproducibility and device consistency\n",
    "# g = torch.Generator(device=device)\n",
    "# g.manual_seed(0) # You can set a seed for reproducibility of shuffling\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle=True, generator = g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29e5a3",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55bf13",
   "metadata": {},
   "source": [
    "#### Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_TransformerEncoder_1 = TransformerEncoderModel(\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=5,\n",
    "    dim_feedforward=256,\n",
    "    input_size=num_feature,\n",
    "    output_size=lookforward_timestep,                # make a prediction with the number of timestep equal to lookforward timestep\n",
    "    seq_len=lookback_timestep,\n",
    "    dropout=0.1,\n",
    ")\n",
    "model_TransformerEncoder_1.to(device)\n",
    "\n",
    "optimizer = Adam(model_TransformerEncoder_1.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "#### Adding an Early stopping and Scheduler into the training process #####\n",
    "early_stop_count = 0\n",
    "patience = 20\n",
    "min_loss = float('inf')\n",
    "\n",
    "# scheduler = ExponentialLR(optimizer, gamma=0.99)        # To adjust learning rates\n",
    "\n",
    "print(\"\\n--- Model ---\")\n",
    "print(\"Model architecture:\")\n",
    "print(model_TransformerEncoder_1)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model_TransformerEncoder_1.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train the Model #####\n",
    "\n",
    "# Clear CUDA cache before starting hyperparameter search\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_TransformerEncoder_1, *loss_history_model_TransformerEncoder = trainer(\n",
    "    model_TransformerEncoder_1, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader,\n",
    "    optimizer=optimizer, \n",
    "    criterion=criterion, \n",
    "    scheduler=None, \n",
    "    max_epochs=200, \n",
    "    early_stopping_patience=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make a Prediction to the whole Dataset #####\n",
    "\n",
    "y_pred, y_test, eval_loss = predictor(\n",
    "    model=model_TransformerEncoder_1, \n",
    "    test_dataloader=test_dataloader,\n",
    "    y_scaler=y_scaler,\n",
    "    criterion=nn.L1Loss(), \n",
    "    )\n",
    "\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "\n",
    "plotly_visualization_line(x=datetime_test, y=[y_test[:, 0], y_pred[:, 0]], name=['Actual', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make a Prediction to the whole Dataset #####\n",
    "\n",
    "y_train_pred, y_train, eval_loss = predictor(\n",
    "    model=model_TransformerEncoder_1, \n",
    "    test_dataloader=train_dataloader,\n",
    "    y_scaler=y_scaler,\n",
    "    criterion=nn.L1Loss(), \n",
    "    )\n",
    "\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "\n",
    "plotly_visualization_line(x=datetime_train, y=[y_train[:, 0], y_train_pred[:, 0]], name=['Actual', 'Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28671f44",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization (Encoder) with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc678aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_bayesian(trial, model_parameters, train_dataloader, val_dataloader, device=cuda_check()):\n",
    "    if torch.cuda.is_available():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    try:\n",
    "        d_model_per_head = trial.suggest_categorical('d_model_per_head', [4, 8, 16, 32, 64])\n",
    "        nhead = trial.suggest_categorical('nhead', [1, 2, 4, 8])\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "        dim_feedforward = trial.suggest_categorical('dim_feedforward', [128, 256, 512, 1024])\n",
    "            \n",
    "        model = TransformerEncoderModel(\n",
    "            input_size=model_parameters['num_feature'],\n",
    "            output_size=model_parameters['lookforward_timestep'],\n",
    "            seq_len=model_parameters['lookback_timestep'],\n",
    "            d_model=d_model_per_head * nhead,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "        optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.L1Loss()\n",
    "        # scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "        _, _, val_loss = trainer(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            scheduler=None,\n",
    "            max_epochs=50,\n",
    "            early_stopping_patience=20,\n",
    "            display_result=False\n",
    "        )\n",
    "\n",
    "        return val_loss[-1] if val_loss else float('inf')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73238073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization\n",
    "n_trials = 60\n",
    "\n",
    "model_parameters = {\n",
    "    'num_feature': num_feature, \n",
    "    'lookforward_timestep': lookforward_timestep, \n",
    "    'lookback_timestep': lookback_timestep\n",
    "}\n",
    "\n",
    "objective = lambda trial: objective_bayesian(\n",
    "        trial, \n",
    "        model_parameters, \n",
    "        train_dataloader, \n",
    "        val_dataloader\n",
    "    )\n",
    " \n",
    "\n",
    "study = run_optimization(\n",
    "    objective = objective, \n",
    "    n_trials = n_trials, \n",
    "    timeout = None, \n",
    "    sampler = optuna.samplers.TPESampler(seed=42), \n",
    "    pruner = optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,     # First 5 trials will run fully without begin pruned\n",
    "        n_warmup_steps=3,       # No pruning in the first 3 steps of any trial\n",
    "        interval_steps=1,       # Pruning check every 2 epochs\n",
    "        n_min_trials=5          # Need at least 3 completed trials before pruning starts\n",
    "        ),\n",
    "    n_jobs = 1,\n",
    "    )\n",
    "\n",
    "final_model_bayesian, r2_bayesian, mse_bayesian, mae_bayesian = train_final_model(study, model_parameters, train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader)\n",
    "best_n_models_baesian = show_n_best_models(study, n=5)\n",
    "print(\"\\nStudy Summary:\")\n",
    "print(f\"Total Trials: {len(study.trials)}\")\n",
    "print(f\"Pruned Trials: {sum(t.state == optuna.trial.TrialState.PRUNED for t in study.trials)}\")\n",
    "print(f\"Completed Trials: {sum(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f74ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_best_model = 5\n",
    "best_n_models_baesian = show_n_best_models(study, n=num_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3328790",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_models_baesian[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71150be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make a Prediction to the whole Dataset #####\n",
    "\n",
    "y_pred, y_test, eval_loss = predictor(\n",
    "    model=final_model_bayesian, \n",
    "    test_dataloader=test_dataloader,\n",
    "    y_scaler=y_scaler,\n",
    "    criterion=nn.L1Loss(), \n",
    "    )\n",
    "\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "\n",
    "plotly_visualization_line(x=datetime_test, y=[y_test[:, 0], y_pred[:, 0]], name=['Actual', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make a Prediction to the whole Dataset #####\n",
    "\n",
    "y_pred, y_test, eval_loss = predictor(\n",
    "    model=final_model_bayesian, \n",
    "    test_dataloader=train_dataloader,\n",
    "    y_scaler=y_scaler,\n",
    "    criterion=nn.L1Loss(), \n",
    "    )\n",
    "\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "\n",
    "plotly_visualization_line(x=datetime_test, y=[y_test[:, 0], y_pred[:, 0]], name=['Actual', 'Predicted'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
